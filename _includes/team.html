<!-- Team Section -->
    <section id="team" class="bg-light-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Predictive Models</h2><br>
                    <div class="text-corpus" align="justify">The final thing we tried to see was whether we can use the data we have in order to predict whether an inspection will pass or fail. We tried 4 different approaches for this problem, however, the main difference is not in the models we used, but in how we extract the features.<br><br>

                    The approach which stood out is using <b>'Term Frequency - Inverse Document Frequency' (TF-IDF) vectorizer</b> to extract important <b>words as features</b> from the violations and comments. This is particularly suited for our dataset because it is robust to the change in violations that occurred in 2018. By training a <b>Linear Support Vector Machine</b> with these features and performing grid search to find the optimal hyperparameters, we obtained a high test accuracy of <b>96%</b>
                    on a 40:60 test-train split of the original dataset. The following 2 maps illustrate a small subsample of the inspections.<br><br>

                    The map on the left illustrates the outcomes predicted by the model on a small subsample of 1412 observations (where an accuracy of <b>98%</b> was attained), while the map on the right shows the ground truth values. Note that the value 1 corresponds to a Pass and 0 corresponds to a Fail.<br><br>

                    <div class="column"><img src="img/map_pred.png" width="50%"><img src="img/map_truth.png" width="50%"></div><br><br>

                    The following confusion matrix supports the results displayed above.<br><br>
                    <div class="column"><img src="img/confusion.png" class="img-responsive img-centered" width="50%"><br><br>

                    On the other hand, the other approaches we attempted provided really good results as well. The first one consisted in converting the <b>violations to binary features</b>: 1 column corresponds to one violation and the value contained is 1 if the violation is resent in the current inspection, 0 otherwise. The limitation of this approach is that it requires us to split the dataset and train 2 different models on the first split containing the old violations and on the new split containing the new violations after 2018. By training a <b>Multilayer Perceptron</b> on the first split of the data (old violations) we obtained a test accuracy of <b>94%</b>.<br><br>

                    The second approach is somewhat similar in the sense that, aside from using only the violations, it also uses <b>other binary features</b> like ‘Inspection Type’, ‘Facility Type’ and the Latitude and Longitude as <b>numerical features</b>. Once again the dataset has to be split and 2 different models have to be trained. By training a simple <b>Logistic Regression</b> on these features we obtained a test accuracy of <b>93%</b>.<br><br>

                    Last but not least, the final approach we used is also similar. The only difference is that we <b>derive some new features</b> to use together with the violations: the <b>result of the previous inspection, the number of days since the last inspection and the number of violations</b>. Once again we trained an <b>MLP</b> and obtained a test accuracy of <b>93%</b>. This was obtained with <b>multiclass labeling</b>.<br><br>

                    The conclusion we arrived at is that, in order to optimize a final predictive model, we should combine all the advantages of the previous models. Moreover, it is indeed possible to predict which food establishments are likely to fail an inspection. This is great news, given that our most important goal was to discover whether such predictive models can be trained on the Chicago Food Inspections data set in order to aid the relatively few inspectors to more effectively and efficiently locate the facilities that truly pose a threat to the Public Health.</div><br><br>

                    <h4 class="section-subheading">Is it possible to generalise our model to other similar datasets from other cities?<br> <br> </h4><br>

                    Since our model seemed to be working very well on the Chicago dataset, we decided to try to generalise it. For this, we picked a dataset that is similar to ours: Boston's food inspection. In this new dataset, we tested our model with the violation description and the inspector's comments. Unfortunately, it did not perform as expected. The overall <b>accuracy</b> achieved was around <b>44%</b>. To figure out the reason behind this poor performance, we extracted and plotted the top 10 TF-IDF features for each class (Pass and Fail) for both Boston's and Chicago's dataset.<br><br>

                    <div class="column"><img src="img/top-tf-idf-chicago.png" width="50%"><img src="img/top-tf-idf-boston.png" width="50%"></div><br><br>


                    The first observation is that the features extracted from the two datasets are very different for the same class. This is one of the explanations behind the poor predictions. Another observation is regarding the two classes for the Boston dataset: the TF-IDF features are very similar. It seems like the comment section of this dataset doesn't have meaningful information regarding the outcome of the inspections. 

                    In conclusion, the generalisation of our model wouldn't work unless the inspections' comments (description of the violations) become more or less unified across cities.<br><br>


                    <h4 class="section-subheading">How can we improve further the Chicago food inspections system?<br> <br> </h4><br>

                    In order to improve the existing inspection system, we thought it would be interesting to prioritize inspection. The food establishments that should be inspected first are the ones that are the more likely to have a critical violation. Indeed, critical violations include violations that may jeopardize the health of customers because they impede food safety. As a result, the establishments that should be inspected first are the ones which have the highest probability of having a critical violation based on their previous inspections. For now, 27% of the inspections are found to have a critical violation. By finding them earlier, we can improve the system and maybe prevent them from happening.<br><br>

                    <div class="column"><img src="img/clustering-1.png" width="50%"><img src="img/clustering-2.png" width="50%"></div><br>

                    To prioritize inspections, one feature that could help our prediction model is to determine if an establishment previously had critical violations and more generally, the category of its previous violations. For now, the clustering is divided into 3 categories before the change in violations and 2 categories after which might have a negative effect on the model. In order to have the same number of categories before and after the change in violations, we need to change a bit the existing clustering. One way to obtain 3 categories is to map Pass and Pass w/conditions together.<br><br>

                    The critical violations identified have a high percentage of failed inspections and a low one of successful inspections. Besides, they are associated with factors that can endanger customers.<br><br>

                    <ul>Here are the features included in our model to predict critical violations:
                        <li>Number of critical/serious/normal violations last time</li>
                        <li>Result of the last inspection</li>
                        <li>Number of violations last time</li>
                        <li>Number of days since the last inspection</li>
                        <li>Risk last time</li>
                        <li>Number of days the establishment has been operating</li>
                    </ul><br><br>

                    <img src="img/priority-distribution-model.png" class="img-responsive img-centered" width="70%"><br><br>

                    Our model has a 77% accuracy. We can create a function which predicts which establishments are to be inspected first at a given date based on their probability of having a critical violation. The above plot is an example for a random date. The yellow dots correspond to the establishments that need to be inspected in priority whereas as we go towards more violet points, we can assume that these establishments can be inspected later since they are less likely to get a critical violation.


                </div>
            </div>
        </div>
    </section>