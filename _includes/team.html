<!-- Team Section -->
    <section id="team" class="bg-light-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Predictive Models</h2><br>
                    <div class="text-corpus" align="justify">The final step we took with our project is trying to see whether we can use the data we have in order to predict whether an inspection will pass or fail. We tried 4 different approaches for this problem, however, the main difference is not in the models we used, but in how we extract the features.<br>

                    The approach which stood out is using <b>'Term Frequency - Inverse Document Frequency' (TF-IDF) vectorizer</b> to extract important <b>words as features</b> from the violations and comments. This is particularly suited for our dataset because it is robust to the change in violations that occurred in 2018. By training a <b>Linear Support Vector Machine</b> with these features and performing grid search to find the optimal hyperparameters, we obtained a high test accuracy of <b>96%</b>.<br><br>

                    On the other hand, the other approaches we attempted provided really good results as well. The first one consisted in converting the <b>violations to binary features</b>: 1 column corresponds to one violation and the value contained is 1 if the violation is resent in the current inspection, 0 otherwise. The limitation of this approach is that it requires us to split the dataset and train 2 different models on the first split containing the old violations and on the new split containing the new violations after 2018. By training a <b>Multilayer Perceptron</b> on the first split of the data (old violations) we obtained a test accuracy of <b>94%</b>.<br><br>

                    The second approach is somewhat similar in the sense that, aside from using only the violations, it also uses <b>other binary features</b> like ‘Inspection Type’, ‘Facility Type’ and the Latitude and Longitude as <b>numerical features</b>. Once again the dataset has to be split and 2 different models have to be trained. By training a simple <b>Logistic Regression</b> on these features we obtained a test accuracy of <b>93%</b>.<br><br>

                    Last but not least, the final approach we used is also similar. The only difference is that we <b>derive some new features</b> to use together with the violations: the <b>result of the previous inspection, the number of days since the last inspection and the number of violations</b>. Once again we trained an <b>MLP</b> and obtained a test accuracy of <b>93%</b>. This was obtained with <b>multiclass labeling</b>.<br><br>

                    The conclusion is that, in order to optimize a final predictive model, we should combine all the advantages of the previous models.</div><br>

                    
                </div>
            </div>
        </div>
    </section>